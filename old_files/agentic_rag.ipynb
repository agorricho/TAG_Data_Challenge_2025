{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import basics\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.agents import create_tool_calling_agent\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import SupabaseVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain import hub\n",
    "\n",
    "from supabase.client import Client, create_client\n",
    "from langchain_core.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://tnjclyviduvzqjkifjch.supabase.co\n",
      "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InRuamNseXZpZHV2enFqa2lmamNoIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDE4MDkwNzUsImV4cCI6MjA1NzM4NTA3NX0.iZbBenrBARtcUX5qn_AP8iL23tDH-bZ6TJr21ml0OvA\n"
     ]
    }
   ],
   "source": [
    "# Specify the path to your .env file if it's not in the same directory\n",
    "dotenv_path = '/home/cgorricho/apps/TAG_Data_Challenge/Agentic-RAG-with-LangChain/.env'\n",
    "\n",
    "# load environment variables\n",
    "load_dotenv(dotenv_path, verbose=True, override=True)  \n",
    "\n",
    "# initiate supabase database\n",
    "supabase_url = os.environ.get(\"SUPABASE_URL\")\n",
    "supabase_key = os.environ.get(\"SUPABASE_SERVICE_KEY\")\n",
    "\n",
    "print(supabase_url)\n",
    "print(supabase_key)\n",
    "\n",
    "supabase: Client = create_client(supabase_url,supabase_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHELL=/bin/bash\n",
      "LANGSMITH_PROJECT=TAG_data_challenge\n",
      "LANGSMITH_TRACING=true\n",
      "PYTHONUNBUFFERED=1\n",
      "CONDA_EXE=/home/cgorricho/anaconda3/bin/conda\n",
      "_CE_M=\n",
      "SUPABASE_SERVICE_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InRuamNseXZpZHV2enFqa2lmamNoIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDE4MDkwNzUsImV4cCI6MjA1NzM4NTA3NX0.iZbBenrBARtcUX5qn_AP8iL23tDH-bZ6TJr21ml0OvA\n",
      "LANGUAGE=en_US:\n",
      "PYTHON_FROZEN_MODULES=on\n",
      "ELECTRON_RUN_AS_NODE=1\n",
      "OPENAI_API_KEY=sk-proj-u0ErCqEkkvXLvCOH7uIiT3BlbkFJGt5dZJQ33nRga1mj0ThB\n",
      "PWD=/home/cgorricho/apps/TAG_Data_Challenge\n",
      "GSETTINGS_SCHEMA_DIR=/home/cgorricho/anaconda3/share/glib-2.0/schemas\n",
      "LOGNAME=cgorricho\n",
      "CONDA_ROOT=/home/cgorricho/anaconda3\n",
      "XDG_SESSION_TYPE=tty\n",
      "CONDA_PREFIX=/home/cgorricho/anaconda3/envs/streamlit\n",
      "LANGSMITH_ENDPOINT=https://api.smith.langchain.com\n",
      "VSCODE_ESM_ENTRYPOINT=vs/workbench/api/node/extensionHostProcess\n",
      "SUPABASE_URL=https://tnjclyviduvzqjkifjch.supabase.co\n",
      "GSETTINGS_SCHEMA_DIR_CONDA_BACKUP=\n",
      "PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING=1\n",
      "_=/usr/bin/env\n",
      "MOTD_SHOWN=pam\n",
      "HOME=/home/cgorricho\n",
      "LANG=en_US.UTF-8\n",
      "LS_COLORS=\n",
      "SSL_CERT_DIR=/usr/lib/ssl/certs\n",
      "FORCE_COLOR=1\n",
      "CONDA_PROMPT_MODIFIER=(streamlit) \n",
      "PYDEVD_USE_FRAME_EVAL=NO\n",
      "VSCODE_AGENT_FOLDER=/home/cgorricho/.vscode-server\n",
      "CLICOLOR=1\n",
      "VSCODE_L10N_BUNDLE_LOCATION=\n",
      "CLICOLOR_FORCE=1\n",
      "SSH_CONNECTION=66.56.63.16 61350 158.220.115.55 22\n",
      "LESSCLOSE=/usr/bin/lesspipe %s %s\n",
      "XDG_SESSION_CLASS=user\n",
      "VSCODE_HANDLES_SIGPIPE=true\n",
      "TERM=xterm-color\n",
      "_CE_CONDA=\n",
      "LESSOPEN=| /usr/bin/lesspipe %s\n",
      "USER=cgorricho\n",
      "GIT_PAGER=cat\n",
      "PYTHONIOENCODING=utf-8\n",
      "CONDA_SHLVL=2\n",
      "SHLVL=1\n",
      "PAGER=cat\n",
      "VSCODE_CWD=/home/cgorricho\n",
      "XDG_SESSION_ID=5956\n",
      "MPLBACKEND=module://matplotlib_inline.backend_inline\n",
      "CONDA_PYTHON_EXE=/home/cgorricho/anaconda3/bin/python\n",
      "XDG_RUNTIME_DIR=/run/user/1000\n",
      "SSL_CERT_FILE=/usr/lib/ssl/certs/ca-certificates.crt\n",
      "SSH_CLIENT=66.56.63.16 61350 22\n",
      "CONDA_DEFAULT_ENV=streamlit\n",
      "CONDA_ALLOW_SOFTLINKS=false\n",
      "VSCODE_CLI_REQUIRE_TOKEN=71d1b985-2287-4840-bb72-a22363f4f422\n",
      "BROWSER=/home/cgorricho/.vscode-server/cli/servers/Stable-6609ac3d66f4eade5cf376d1cb76f13985724bcb/server/bin/helpers/browser.sh\n",
      "PATH=/home/cgorricho/anaconda3/envs/streamlit/bin:/home/cgorricho/.vscode-server/cli/servers/Stable-6609ac3d66f4eade5cf376d1cb76f13985724bcb/server/bin/remote-cli:/home/cgorricho/anaconda3/envs/streamlit/bin:/home/cgorricho/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\n",
      "LANGSMITH_API_KEY=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\n",
      "DBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/1000/bus\n",
      "VSCODE_NLS_CONFIG={\"userLocale\":\"en\",\"osLocale\":\"en\",\"resolvedLanguage\":\"en\",\"defaultMessagesFile\":\"/home/cgorricho/.vscode-server/cli/servers/Stable-6609ac3d66f4eade5cf376d1cb76f13985724bcb/server/out/nls.messages.json\",\"locale\":\"en\",\"availableLanguages\":{}}\n",
      "CONDA_PREFIX_1=/home/cgorricho/anaconda3\n",
      "VSCODE_HANDLES_UNCAUGHT_ERRORS=true\n",
      "VSCODE_IPC_HOOK_CLI=/run/user/1000/vscode-ipc-7dfc98fd-0b5a-4bcb-8909-a5bdb789e407.sock\n"
     ]
    }
   ],
   "source": [
    "!env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate embeddings model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# initiate vector store\n",
    "vector_store = SupabaseVectorStore(\n",
    "    embedding=embeddings,\n",
    "    client=supabase,\n",
    "    table_name=\"documents\",\n",
    "    query_name=\"match_documents\",\n",
    ")\n",
    "\n",
    "# initiate large language model (temperature = 0)\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# fetch the prompt from the prompt hub\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "\n",
    "# create the tools\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query: str):\n",
    "    \"\"\"Retrieve information related to a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')]\n"
     ]
    }
   ],
   "source": [
    "print(prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a helpful assistant'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.messages[0].prompt.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt.messages[0].prompt.template = 'you are the ruler of the world'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x7fe52b31f9c0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]], 'agent_scratchpad': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x7fe52b31f9c0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-functions-agent', 'lc_hub_commit_hash': 'a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='you are the ruler of the world'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the tools and provide to the llm\n",
    "tools = [retrieve]\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "# create the agent executor\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `retrieve` with `{'query': 'naive RAG'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mSource: {'page': 3, 'source': 'documents/agentic rag paper.pdf'}\n",
      "Content: Figure 2: Core Components of RAG\n",
      "limitations. By understanding the evolution of these paradigms, readers can appreciate the advancements made in\n",
      "retrieval and generative capabilities and their application in various domains\n",
      "2.3.1 Naïve RAG\n",
      "Naïve RAG [20] represents the foundational implementation of retrieval-augmented generation. Figure 3 illustrates the\n",
      "simple retrieve-read workflow of Naive RAG, focusing on keyword-based retrieval and static datasets.. These systems\n",
      "rely on simple keyword-based retrieval techniques, such as TF-IDF and BM25, to fetch documents from static datasets.\n",
      "The retrieved documents are then used to augment the language model’s generative capabilities.\n",
      "Figure 3: An Overview of Naive RAG.\n",
      "Naïve RAG is characterized by its simplicity and ease of implementation, making it suitable for tasks involving\n",
      "fact-based queries with minimal contextual complexity. However, it suffers from several limitations:\n",
      "\n",
      "Source: {'page': 1, 'source': 'documents/agentic rag paper.pdf'}\n",
      "Content: processing. Unlike traditional RAG, Agentic RAG employs autonomous agents to orchestrate retrieval, filter relevant\n",
      "information, and refine responses, excelling in scenarios requiring precision and adaptability. The overview of Agentic\n",
      "RAG is in figure 1.\n",
      "This survey explores the foundational principles, taxonomy, and applications of Agentic RAG. It provides a comprehen-\n",
      "sive overview of RAG paradigms, such as Naïve RAG, Modular RAG, and Graph RAG [16], alongside their evolution\n",
      "into Agentic RAG systems. Key contributions include a detailed taxonomy of Agentic RAG frameworks, applications\n",
      "across domains such as healthcare [17, 18], finance, and education [19], and insights into implementation strategies,\n",
      "benchmarks, and ethical considerations.\n",
      "The structure of this paper is as follows: Section 2 introduces RAG and its evolution, highlighting the limitations of\n",
      "traditional approaches. Section 3 elaborates on the principles of agentic intelligence and agentic patterns. Section 4\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `retrieve` with `{'query': 'agentic RAG'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mSource: {'page': 1, 'source': 'documents/agentic rag paper.pdf'}\n",
      "Content: processing. Unlike traditional RAG, Agentic RAG employs autonomous agents to orchestrate retrieval, filter relevant\n",
      "information, and refine responses, excelling in scenarios requiring precision and adaptability. The overview of Agentic\n",
      "RAG is in figure 1.\n",
      "This survey explores the foundational principles, taxonomy, and applications of Agentic RAG. It provides a comprehen-\n",
      "sive overview of RAG paradigms, such as Naïve RAG, Modular RAG, and Graph RAG [16], alongside their evolution\n",
      "into Agentic RAG systems. Key contributions include a detailed taxonomy of Agentic RAG frameworks, applications\n",
      "across domains such as healthcare [17, 18], finance, and education [19], and insights into implementation strategies,\n",
      "benchmarks, and ethical considerations.\n",
      "The structure of this paper is as follows: Section 2 introduces RAG and its evolution, highlighting the limitations of\n",
      "traditional approaches. Section 3 elaborates on the principles of agentic intelligence and agentic patterns. Section 4\n",
      "\n",
      "Source: {'page': 8, 'source': 'documents/agentic rag paper.pdf'}\n",
      "Content: resulting in Agentic RAG. By incorporating autonomous agents capable of dynamic decision-making, iterative reasoning,\n",
      "and adaptive retrieval strategies, Agentic RAG builds on the modularity of earlier paradigms while overcoming their\n",
      "inherent constraints. This evolution enables more complex, multi-domain tasks to be addressed with enhanced precision\n",
      "and contextual understanding, positioning Agentic RAG as a cornerstone for next-generation AI applications. In\n",
      "particular, Agentic RAG systems reduce latency through optimized workflows and refine outputs iteratively, tackling\n",
      "the very challenges that have historically hindered traditional RAG’s scalability and effectiveness.\n",
      "3 Core Principles and Background of Agentic Intelligence\n",
      "Agentic Intelligence forms the foundation of Agentic Retrieval-Augmented Generation (RAG) systems, enabling them\n",
      "to transcend the static and reactive nature of traditional RAG. By integrating autonomous agents capable of dynamic\u001b[0m\u001b[32;1m\u001b[1;3mThe difference between naive RAG and agentic RAG lies in their implementation and capabilities:\n",
      "\n",
      "1. Naive RAG:\n",
      "   - Naive RAG represents the foundational implementation of retrieval-augmented generation.\n",
      "   - It relies on simple keyword-based retrieval techniques, such as TF-IDF and BM25, to fetch documents from static datasets.\n",
      "   - Naive RAG is characterized by its simplicity and ease of implementation, making it suitable for tasks involving fact-based queries with minimal contextual complexity.\n",
      "\n",
      "2. Agentic RAG:\n",
      "   - Agentic RAG employs autonomous agents to orchestrate retrieval, filter relevant information, and refine responses.\n",
      "   - It excels in scenarios requiring precision and adaptability, unlike traditional RAG.\n",
      "   - Agentic RAG systems incorporate autonomous agents capable of dynamic decision-making, iterative reasoning, and adaptive retrieval strategies.\n",
      "   - This evolution enables more complex, multi-domain tasks to be addressed with enhanced precision and contextual understanding, positioning Agentic RAG as a cornerstone for next-generation AI applications.\n",
      "\n",
      "In summary, while naive RAG is simple and suitable for basic tasks, agentic RAG is more advanced, utilizing autonomous agents for dynamic decision-making and adaptability in addressing complex tasks with precision.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# invoke the agent\n",
    "response = agent_executor.invoke({\"input\": \"What is the difference between naive RAG and agentic RAG\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Which of agentic RAG or naive RAG do scholars prefer',\n",
       " 'output': 'Scholars prefer Agentic RAG over Naive RAG. Agentic RAG employs autonomous agents to orchestrate retrieval, filter relevant information, and refine responses, excelling in scenarios requiring precision and adaptability. It transcends the limitations of traditional RAG systems by embedding autonomous AI agents into the RAG pipeline, enabling unparalleled flexibility, scalability, and context-awareness across diverse applications.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
